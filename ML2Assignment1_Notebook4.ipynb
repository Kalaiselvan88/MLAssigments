{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ML2Assignment1_Notebook4.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalaiselvan88/MLAssignments/blob/main/ML2Assignment1_Notebook4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo49-cP4SU-W"
      },
      "source": [
        "# Setting the environment variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k42aXCMoSU-Y"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook --no-browser\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
        "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B36XDomSU-Z"
      },
      "source": [
        "# Ecommerce Churn Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Oza3qeSU-Z"
      },
      "source": [
        "The aim of the assignment is to build a model that predicts whether a person purchases an item after it has been added to the cart or not. Being a classification problem, you are expected to use your understanding of all the three models covered till now. You must select the most robust model and provide a solution that predicts the churn in the most suitable manner. \n",
        "\n",
        "For this assignment, you are provided the data associated with an e-commerce company for the month of October 2019. Your task is to first analyse the data, and then perform multiple steps towards the model building process.\n",
        "\n",
        "The broad tasks are:\n",
        "- Data Exploration\n",
        "- Feature Engineering\n",
        "- Model Selection\n",
        "- Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4gZxvqRSU-Z"
      },
      "source": [
        "### Data description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwEgNLSLSU-a"
      },
      "source": [
        "The dataset stores the information of a customer session on the e-commerce platform. It records the activity and the associated parameters with it.\n",
        "\n",
        "- **event_time**: Date and time when user accesses the platform\n",
        "- **event_type**: Action performed by the customer\n",
        "            - View\n",
        "            - Cart\n",
        "            - Purchase\n",
        "            - Remove from cart\n",
        "- **product_id**: Unique number to identify the product in the event\n",
        "- **category_id**: Unique number to identify the category of the product\n",
        "- **category_code**: Stores primary and secondary categories of the product\n",
        "- **brand**: Brand associated with the product\n",
        "- **price**: Price of the product\n",
        "- **user_id**: Unique ID for a customer\n",
        "- **user_session**: Session ID for a user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf5xHcUiSU-a"
      },
      "source": [
        "### Initialising the SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPoZbr-OSU-a"
      },
      "source": [
        "The dataset provided is 5 GBs in size. Therefore, it is expected that you increase the driver memory to a greater number. You can refer to notebook 1 for the steps involved here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AONPtXcsSU-b",
        "outputId": "2dc0ed8d-f1d8-44d9-c8b4-20ec04404991"
      },
      "source": [
        "# initialising the session with 14 GB driver memory\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "MAX_MEMORY = \"14G\"\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"random forests\") \\\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.catalog.clearCache()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ip-172-31-90-29.ec2.internal:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>random forests</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f31ec921c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av8CTV_9SU-b",
        "outputId": "f8d92cd8-651a-48e6-c090-d6ad5a03e7f1"
      },
      "source": [
        "# Loading the clean data\n",
        "df = spark.read.parquet('cleaned_df.parquet')\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28650604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT6pA246SU-c"
      },
      "source": [
        "## Task 3: Model Selection\n",
        "3 models for classification:\t\n",
        "- Logistic Regression\n",
        "- Decision Tree\n",
        "- Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp2wIilSU-c"
      },
      "source": [
        "### Model 3: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47uHzyX4SU-c",
        "outputId": "91eb8ca8-ddd5-4c92-df79-343157c8b15a"
      },
      "source": [
        "# Additional steps for Random Forests, if any\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- cat_l1: string (nullable = true)\n",
            " |-- cat_l2: string (nullable = true)\n",
            " |-- hour_bucket: double (nullable = true)\n",
            " |-- brand_red: string (nullable = true)\n",
            " |-- is_purchased: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwPVJrzPSU-d"
      },
      "source": [
        "#### Feature Transformation (Code will be same; check for the columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMU1zZ6XSU-d",
        "outputId": "7259419e-0fd8-4bd2-a8c0-65d64aad0a67"
      },
      "source": [
        "# Check if only the required columns are present to build the model\n",
        "# If not, drop the redundant columns\n",
        "\n",
        "# Here we are dropping category_id since we have the category details in cat_l1 and cat_l2\n",
        "# We are also dropping user_session since it has many unique values which might take a lot of time\n",
        "# to do OneHotEncoder and moreover it is not very much helpful for Churn prediction and has already been used for EDA\n",
        "df_rand_forst = df.drop('category_id', 'user_session')\n",
        "df_rand_forst = df.withColumnRenamed('is_purchased', 'label')\n",
        "df_rand_forst = df_rand_forst.dropDuplicates()\n",
        "df_rand_forst.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- cat_l1: string (nullable = true)\n",
            " |-- cat_l2: string (nullable = true)\n",
            " |-- hour_bucket: double (nullable = true)\n",
            " |-- brand_red: string (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRFqwK9USU-e"
      },
      "source": [
        "# Feature transformation for categorical features\n",
        "#import the string indexer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "#import the onehot encoder\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "\n",
        "si1 = StringIndexer(inputCol= 'cat_l1', outputCol='cat_l1_ix')\n",
        "si2 = StringIndexer(inputCol= 'cat_l2', outputCol='cat_l2_ix')\n",
        "si3 = StringIndexer(inputCol= 'brand_red', outputCol='brand_red_ix')\n",
        "\n",
        "cat_indx = ['cat_l1_ix','cat_l2_ix','brand_red_ix','day_of_week','hour_bucket']\n",
        "\n",
        "ohe = OneHotEncoderEstimator(inputCols=cat_indx,\n",
        "                             outputCols=['cat_l1_en','cat_l2_en','brand_red_en','day_of_week_en','hour_bucket_en'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BMKzZF9SU-e"
      },
      "source": [
        "# Vector assembler to combine all the features\n",
        "#import the vector assembler \n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "cols = ['price','cat_l1_en','cat_l2_en','brand_red_en','day_of_week_en','hour_bucket_en']\n",
        "assembler = VectorAssembler(inputCols=cols,\n",
        "                            outputCol=\"features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuc1FKslSU-f"
      },
      "source": [
        "# Pipeline for the tasks\n",
        "# import pipline API\n",
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsypb6v0SU-f",
        "outputId": "229c99c3-0b18-4a6d-fc97-af6114961a38"
      },
      "source": [
        "# Transforming the dataframe df\n",
        "#create the pipeline object\n",
        "pipeline = Pipeline(stages=[si1, si2, si3, ohe, assembler])\n",
        "\n",
        "#use the object to transform the dataframe \n",
        "df_rf_encoded = pipeline.fit(df_rand_forst).transform(df_rand_forst)\n",
        "df_rf_encoded.select(\"features\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(101,[0,8,24,71,9...|\n",
            "|(101,[0,10,33,71,...|\n",
            "|(101,[0,1,15,75,9...|\n",
            "|(101,[0,5,18,71,9...|\n",
            "|(101,[0,1,15,73,9...|\n",
            "|(101,[0,1,19,75,9...|\n",
            "|(101,[0,2,14,71,9...|\n",
            "|(101,[0,3,16,71,9...|\n",
            "|(101,[0,1,15,73,9...|\n",
            "|(101,[0,1,19,71,9...|\n",
            "|(101,[0,2,14,71,1...|\n",
            "|(101,[0,6,25,71,9...|\n",
            "|(101,[0,1,15,75,9...|\n",
            "|(101,[0,4,28,78],...|\n",
            "|(101,[0,3,21,71,9...|\n",
            "|(101,[0,6,26,71,9...|\n",
            "|(101,[0,1,22,72,9...|\n",
            "|(101,[0,3,16,89,9...|\n",
            "|(101,[0,5,18,71,9...|\n",
            "|(101,[0,3,35,71,9...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TzLMnfiSU-g",
        "outputId": "de882b20-7897-4d12-feb1-0b9b9cfe93d4"
      },
      "source": [
        "# Schema of the transformed df\n",
        "df_rf_encoded.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- cat_l1: string (nullable = true)\n",
            " |-- cat_l2: string (nullable = true)\n",
            " |-- hour_bucket: double (nullable = true)\n",
            " |-- brand_red: string (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- cat_l1_ix: double (nullable = false)\n",
            " |-- cat_l2_ix: double (nullable = false)\n",
            " |-- brand_red_ix: double (nullable = false)\n",
            " |-- cat_l1_en: vector (nullable = true)\n",
            " |-- day_of_week_en: vector (nullable = true)\n",
            " |-- hour_bucket_en: vector (nullable = true)\n",
            " |-- brand_red_en: vector (nullable = true)\n",
            " |-- cat_l2_en: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9D_4YsSU-g",
        "outputId": "774473eb-2f9e-4a04-d006-6247739cf5d8"
      },
      "source": [
        "# Checking the elements of the transformed df - Top 20 rows\n",
        "df_rf_encoded.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+-------+---------+--------------------+-----------+------------+-----------+-----------+---------+-----+---------+---------+------------+--------------+--------------+--------------+---------------+---------------+--------------------+\n",
            "|product_id|        category_id|  price|  user_id|        user_session|day_of_week|      cat_l1|     cat_l2|hour_bucket|brand_red|label|cat_l1_ix|cat_l2_ix|brand_red_ix|     cat_l1_en|day_of_week_en|hour_bucket_en|   brand_red_en|      cat_l2_en|            features|\n",
            "+----------+-------------------+-------+---------+--------------------+-----------+------------+-----------+-----------+---------+-----+---------+---------+------------+--------------+--------------+--------------+---------------+---------------+--------------------+\n",
            "|  27700139|2053013560086233771|  35.39|539259746|25c30a37-eebd-464...|          2|construction|      tools|        1.0|   others|    0|      7.0|     10.0|         0.0|(13,[7],[1.0])| (7,[2],[1.0])| (3,[1],[1.0])| (20,[0],[1.0])|(57,[10],[1.0])|(101,[0,8,24,71,9...|\n",
            "|  28400709|2053013566209917945|  72.07|558350018|d3afb296-bd98-4d7...|          4| accessories|        bag|        0.0|   others|    0|      9.0|     19.0|         0.0|(13,[9],[1.0])| (7,[4],[1.0])| (3,[0],[1.0])| (20,[0],[1.0])|(57,[19],[1.0])|(101,[0,10,33,71,...|\n",
            "|   1004565|2053013555631882655|  169.6|524340259|f3ee67b4-e525-4ab...|          5| electronics| smartphone|        0.0|   huawei|    0|      0.0|      1.0|         4.0|(13,[0],[1.0])| (7,[5],[1.0])| (3,[0],[1.0])| (20,[4],[1.0])| (57,[1],[1.0])|(101,[0,1,15,75,9...|\n",
            "|  28717989|2053013565639492569| 122.27|513930718|8283d850-ef28-499...|          3|     apparel|      shoes|        3.0|   others|    0|      4.0|      4.0|         0.0|(13,[4],[1.0])| (7,[3],[1.0])|     (3,[],[])| (20,[0],[1.0])| (57,[4],[1.0])|(101,[0,5,18,71,9...|\n",
            "|   1002524|2053013555631882655| 513.43|537759739|fe07b361-bb8d-4a9...|          4| electronics| smartphone|        3.0|    apple|    1|      0.0|      1.0|         2.0|(13,[0],[1.0])| (7,[4],[1.0])|     (3,[],[])| (20,[2],[1.0])| (57,[1],[1.0])|(101,[0,1,15,73,9...|\n",
            "|   5100721|2053013553341792533|  179.9|540971209|f29f5fe3-9ea0-446...|          3| electronics|     clocks|        3.0|   huawei|    0|      0.0|      5.0|         4.0|(13,[0],[1.0])| (7,[3],[1.0])|     (3,[],[])| (20,[4],[1.0])| (57,[5],[1.0])|(101,[0,1,19,75,9...|\n",
            "|  12702399|2053013553559896355|  72.05|530385282|3cf20d5f-7d46-487...|          3|          NA|         NA|        0.0|   others|    0|      1.0|      0.0|         0.0|(13,[1],[1.0])| (7,[3],[1.0])| (3,[0],[1.0])| (20,[0],[1.0])| (57,[0],[1.0])|(101,[0,2,14,71,9...|\n",
            "|   3601405|2053013563810775923| 180.16|514560037|492a94a1-b19b-40f...|          4|  appliances|    kitchen|        2.0|   others|    0|      2.0|      2.0|         0.0|(13,[2],[1.0])| (7,[4],[1.0])| (3,[2],[1.0])| (20,[0],[1.0])| (57,[2],[1.0])|(101,[0,3,16,71,9...|\n",
            "|   1004239|2053013555631882655|1334.63|520811195|12bde7a6-9a54-44e...|          4| electronics| smartphone|        3.0|    apple|    0|      0.0|      1.0|         2.0|(13,[0],[1.0])| (7,[4],[1.0])|     (3,[],[])| (20,[2],[1.0])| (57,[1],[1.0])|(101,[0,1,15,73,9...|\n",
            "|  21406710|2053013561579406073| 643.52|535135046|a2c4d7d1-c3e3-400...|          2| electronics|     clocks|        0.0|   others|    0|      0.0|      5.0|         0.0|(13,[0],[1.0])| (7,[2],[1.0])| (3,[0],[1.0])| (20,[0],[1.0])| (57,[5],[1.0])|(101,[0,1,19,71,9...|\n",
            "|   3300317|2053013555355058573|  59.18|512457557|630599fb-9919-4eb...|          7|          NA|         NA|        2.0|   others|    0|      1.0|      0.0|         0.0|(13,[1],[1.0])|     (7,[],[])| (3,[2],[1.0])| (20,[0],[1.0])| (57,[0],[1.0])|(101,[0,2,14,71,1...|\n",
            "|  17200917|2053013559792632471|1029.22|545492819|577bbfd8-11e2-43d...|          3|   furniture|living_room|        3.0|   others|    0|      5.0|     11.0|         0.0|(13,[5],[1.0])| (7,[3],[1.0])|     (3,[],[])| (20,[0],[1.0])|(57,[11],[1.0])|(101,[0,6,25,71,9...|\n",
            "|   1004565|2053013555631882655| 169.48|526621871|3de4ede4-f6b5-4a1...|          2| electronics| smartphone|        1.0|   huawei|    1|      0.0|      1.0|         4.0|(13,[0],[1.0])| (7,[2],[1.0])| (3,[1],[1.0])| (20,[4],[1.0])| (57,[1],[1.0])|(101,[0,1,15,75,9...|\n",
            "|   1701082|2053013553031414015| 280.29|557965991|d164dae1-d180-4db...|          7|   computers|peripherals|        3.0|       lg|    0|      3.0|     14.0|         7.0|(13,[3],[1.0])|     (7,[],[])|     (3,[],[])| (20,[7],[1.0])|(57,[14],[1.0])|(101,[0,4,28,78],...|\n",
            "|   3700600|2053013565983425517|  51.43|514062275|5d584a78-e7af-492...|          1|  appliances|environment|        0.0|   others|    0|      2.0|      7.0|         0.0|(13,[2],[1.0])| (7,[1],[1.0])| (3,[0],[1.0])| (20,[0],[1.0])| (57,[7],[1.0])|(101,[0,3,21,71,9...|\n",
            "|  13200969|2053013557192163841| 102.94|549500310|ad311ee8-8725-4b4...|          6|   furniture|    bedroom|        2.0|   others|    0|      5.0|     12.0|         0.0|(13,[5],[1.0])| (7,[6],[1.0])| (3,[2],[1.0])| (20,[0],[1.0])|(57,[12],[1.0])|(101,[0,6,26,71,9...|\n",
            "|   1801689|2053013554415534427| 263.58|552294449|077d92df-ce84-485...|          6| electronics|      video|        2.0|  samsung|    0|      0.0|      8.0|         1.0|(13,[0],[1.0])| (7,[6],[1.0])| (3,[2],[1.0])| (20,[1],[1.0])| (57,[8],[1.0])|(101,[0,1,22,72,9...|\n",
            "|   3200549|2053013555321504139|  25.71|521838455|8a11c631-8e52-450...|          3|  appliances|    kitchen|        0.0| dauscher|    0|      2.0|      2.0|        18.0|(13,[2],[1.0])| (7,[3],[1.0])| (3,[0],[1.0])|(20,[18],[1.0])| (57,[2],[1.0])|(101,[0,3,16,89,9...|\n",
            "|  28720337|2053013565069067197| 118.15|536608563|9612cb69-d4b7-467...|          4|     apparel|      shoes|        3.0|   others|    0|      4.0|      4.0|         0.0|(13,[4],[1.0])| (7,[4],[1.0])|     (3,[],[])| (20,[0],[1.0])| (57,[4],[1.0])|(101,[0,5,18,71,9...|\n",
            "|   3800545|2053013566176363511| 190.46|540047124|6a5574d9-680b-43d...|          1|  appliances|       iron|        0.0|   others|    0|      2.0|     21.0|         0.0|(13,[2],[1.0])| (7,[1],[1.0])| (3,[0],[1.0])| (20,[0],[1.0])|(57,[21],[1.0])|(101,[0,3,35,71,9...|\n",
            "+----------+-------------------+-------+---------+--------------------+-----------+------------+-----------+-----------+---------+-----+---------+---------+------------+--------------+--------------+--------------+---------------+---------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pi3Y4czSU-h"
      },
      "source": [
        "#### Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRnqduyhSU-h"
      },
      "source": [
        "# Splitting the data into train and test (Remember you are expected to compare the model later)\n",
        "df_rf_train, df_rf_test = df_rf_encoded.randomSplit([0.7,0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPtUmBC6SU-h",
        "outputId": "779fe70e-2fa0-45f5-9b04-709eb8c3c5ac"
      },
      "source": [
        "# Number of rows in train and test data\n",
        "df_rf_train.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20053562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SllC33t2SU-i",
        "outputId": "027282f7-1023-455d-d861-3d709db8fbe1"
      },
      "source": [
        "df_rf_test.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8597042"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYp3upgESU-i"
      },
      "source": [
        "#### Let us cache both train and test data since it is huge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf7yhonSSU-i",
        "outputId": "92e175c6-318d-428e-82ba-807e28c166a1"
      },
      "source": [
        "df_rf_train.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: int, category_id: bigint, price: double, user_id: int, user_session: string, day_of_week: int, cat_l1: string, cat_l2: string, hour_bucket: double, brand_red: string, label: int, cat_l1_ix: double, cat_l2_ix: double, brand_red_ix: double, cat_l1_en: vector, day_of_week_en: vector, hour_bucket_en: vector, brand_red_en: vector, cat_l2_en: vector, features: vector]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e3BVw8LSU-j",
        "outputId": "c7acc171-f583-4732-91a5-4692b5401a15"
      },
      "source": [
        "df_rf_test.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: int, category_id: bigint, price: double, user_id: int, user_session: string, day_of_week: int, cat_l1: string, cat_l2: string, hour_bucket: double, brand_red: string, label: int, cat_l1_ix: double, cat_l2_ix: double, brand_red_ix: double, cat_l1_en: vector, day_of_week_en: vector, hour_bucket_en: vector, brand_red_en: vector, cat_l2_en: vector, features: vector]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IvN4a4lSU-j"
      },
      "source": [
        "#### Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmE7erqQSU-j"
      },
      "source": [
        "# Building the model with hyperparameter tuning\n",
        "# Create ParamGrid for Cross Validation\n",
        "\n",
        "#Import the libraries required:\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Create initial Random Forest Model\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
        "\n",
        "# Create ParamGrid for Cross Validation\n",
        "rfparamGrid = (ParamGridBuilder()\n",
        "               .addGrid(rf.maxDepth, [2, 10, 30])\n",
        "               .addGrid(rf.numTrees, [10, 30, 50])\n",
        "               .addGrid(rf.impurity, ['gini','entropy'])\n",
        "               .build())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoCanZ_tSU-j"
      },
      "source": [
        "# Run cross-validation steps\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Evaluate model\n",
        "rfevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "rfcv = CrossValidator(estimator = rf,\n",
        "                      estimatorParamMaps = rfparamGrid,\n",
        "                      evaluator = rfevaluator,\n",
        "                      numFolds = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "megfKtBDSU-k"
      },
      "source": [
        "# Fitting the models on transformed df\n",
        "\n",
        "# Run cross validations\n",
        "rfcvModel = rfcv.fit(df_rf_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Zbq2nJSU-k"
      },
      "source": [
        "# Best model from the results of cross-validation\n",
        "rfcvModel.bestModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYhU2vY5SU-k"
      },
      "source": [
        "#### The above random forest hyper parameter tuning took almost 15 hours and then the EC2 server crashed. Hence let us try building a single random forest with 30 decision tress and check its metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r3JWSQySU-l"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', \\\n",
        "                            maxDepth=5, impurity='gini', numTrees=30, seed=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TDtSATHSU-m"
      },
      "source": [
        "# Fitting the model over the training set\n",
        "rfmodel = rf.fit(df_rf_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8cY_DeYSU-n"
      },
      "source": [
        "#### Model Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ywPztASU-n"
      },
      "source": [
        "Required Steps:\n",
        "- Fit on test data\n",
        "- Performance analysis\n",
        "    - Appropriate Metric with reasoning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlqcYvKtSU-o"
      },
      "source": [
        "# Applying the model on test set\n",
        "rfpredictions = rfmodel.transform(df_rf_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ci9-f-TSU-o",
        "outputId": "444c1848-7179-42c7-8576-e7fb981d8eda"
      },
      "source": [
        "# Printing the required columns\n",
        "rfpredictions.select('label', 'rawPrediction', 'prediction', 'probability').show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+----------+--------------------+\n",
            "|label|       rawPrediction|prediction|         probability|\n",
            "+-----+--------------------+----------+--------------------+\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "|    0|[29.2764741674940...|       0.0|[0.97588247224980...|\n",
            "+-----+--------------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FcGzkRBSU-o"
      },
      "source": [
        "# Model evaluation\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_tOuXSvSU-o",
        "outputId": "0b313bf0-0064-4e6c-d889-b9068d79182a"
      },
      "source": [
        "# Check accuracy\n",
        "evaluator.evaluate(rfpredictions, {evaluator.metricName: \"accuracy\"})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9758568121453868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFui0TBUSU-p"
      },
      "source": [
        "#### As we can see we get a pretty decent accuracy with random forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SRcUWNvSU-p",
        "outputId": "aff2cc36-50f5-45fd-c45d-3f0c0fd25cb4"
      },
      "source": [
        "# Check F1 score\n",
        "evaluator.evaluate(rfpredictions, {evaluator.metricName: \"f1\"})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.963932722206274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXYHPP_6SU-p",
        "outputId": "9b6d00a5-547b-4815-98f6-624e9700bfa0"
      },
      "source": [
        "# Check weighted recall\n",
        "evaluator.evaluate(rfpredictions, {evaluator.metricName: \"weightedRecall\"})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9758568121453868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4tzHWzTSU-q",
        "outputId": "db3001ff-143b-49d3-e6bf-e5b2dc59cc10"
      },
      "source": [
        "# Feature Importance\n",
        "rfmodel.featureImportances"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(101, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvB75XzSU-q"
      },
      "source": [
        "# Defining a function to extract features along with the feature importance score\n",
        "import pandas as pd\n",
        "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
        "    list_extract = []\n",
        "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
        "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
        "    varlist = pd.DataFrame(list_extract)\n",
        "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
        "    return(varlist.sort_values('score', ascending = False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PubZzBM7SU-q"
      },
      "source": [
        "# Printing the feature importance scores\n",
        "featureImportdf = ExtractFeatureImp(rfmodel.featureImportances, rfpredictions, \"features\")\n",
        "feaimpdf = spark.createDataFrame(featureImportdf)\n",
        "feaimpdf.coalesce(1).write.option(\"header\", \"true\").parquet(\"feature_importances.parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA-ycMJhSU-q"
      },
      "source": [
        "#### Summary of the best Random Forest model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvjH-_IcSU-r"
      },
      "source": [
        "#### As we can see a random forest model with 30 trees gives a decent performance. However we can increase the depth and also number of trees if needed. It is purely based on business requirement to decide on the best random forest model after increasing the number of trees further"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjDjexgqSU-r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATS6NUPESU-r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}